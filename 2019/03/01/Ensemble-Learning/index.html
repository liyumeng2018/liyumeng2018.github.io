<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="集成学习机器学习中的一类方法，有时也被称为“多分类器系统（multi-classifier system）”、基于委员会的学习（Committee-based learning）。它对多个机器学习模型进行组合形成一个精度更高的模型，参与组合的模型称为弱学习器（weak learner）。在预测时使用弱学习器模型联合起来进行预测；训练时需要用训练样本集依次训练出这些弱学习器。典型的集成学习算法是随机">
<meta property="og:type" content="article">
<meta property="og:title" content="Ensemble Learning">
<meta property="og:url" content="http://yoursite.com/2019/03/01/Ensemble-Learning/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="集成学习机器学习中的一类方法，有时也被称为“多分类器系统（multi-classifier system）”、基于委员会的学习（Committee-based learning）。它对多个机器学习模型进行组合形成一个精度更高的模型，参与组合的模型称为弱学习器（weak learner）。在预测时使用弱学习器模型联合起来进行预测；训练时需要用训练样本集依次训练出这些弱学习器。典型的集成学习算法是随机">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-03-01T02:50:30.528Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ensemble Learning">
<meta name="twitter:description" content="集成学习机器学习中的一类方法，有时也被称为“多分类器系统（multi-classifier system）”、基于委员会的学习（Committee-based learning）。它对多个机器学习模型进行组合形成一个精度更高的模型，参与组合的模型称为弱学习器（weak learner）。在预测时使用弱学习器模型联合起来进行预测；训练时需要用训练样本集依次训练出这些弱学习器。典型的集成学习算法是随机">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/03/01/Ensemble-Learning/">





  <title>Ensemble Learning | Hexo</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/01/Ensemble-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Ensemble Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-01T10:37:43+08:00">
                2019-03-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><p>机器学习中的一类方法，有时也被称为“多分类器系统（multi-classifier system）”、基于委员会的学习（Committee-based learning）。它对多个机器学习模型进行组合形成一个精度更高的模型，参与组合的模型称为弱学习器（weak learner）。在预测时使用弱学习器模型联合起来进行预测；训练时需要用训练样本集依次训练出这些弱学习器。典型的集成学习算法是随机森林和boosting算法，而AdaBoost算法是boosting算法的一种实现版本。</p>
<p>弱学习器：常指泛化性能略优于随机猜测的学习器，例如在二分类问题精度略高于50%的分类器。</p>
<p>一般来说集成学习可以分为三大类：</p>
<ul>
<li>用于减少方差的bagging</li>
<li>用于减少偏差的boosting</li>
<li>用于提升预测结果的stacking</li>
</ul>
<p>集成学习方法也可以归为如下两大类：串行集成方法，这种方法串行地生成基础模型（如AdaBoost）。串行集成的基本动机是利用基础模型之间的依赖。通过给错分样本一个较大的权重来提升性能。并行集成方法，这种方法并行地生成基础模型（如Random Forest）。并行集成的基本动机是利用基础模型的独立性，因为通过平均能够较大地降低误差。</p>
<h3 id="个体学习器"><a href="#个体学习器" class="headerlink" title="个体学习器"></a>个体学习器</h3><p>个体学习器通常是用一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等。集成中只包含同种类型的个体学习器，例如“决策树集成”中的个体学习器全是决策树，“神经网络集成”中就全是神经网络，这样的集成是“同质”（homogeneous）的，同质集成中的个体学习器也称为“基学习器”（base learner），相应的学习算法称为“基学习算法”（base learning algorithm）。有同质就有异质（heterogeneous），若集成包含不同类型的个体学习器，例如同时包含决策树和神经网络，那么这时个体学习器一般不称为基学习器，而称作“组件学习器”（component leaner）或直接称为个体学习器。</p>
<p>要获得好的集成，<strong>个体学习器应“好而不同”</strong>，即个体学习器要有一定的准确性，即学习器不能太坏，并且要有“多样性”（diversity），即学习器间具有差异。</p>
<p>根据个体学习器生成方式的不同，目前集成学习方法大致可分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是和Bagging和“随机森林”（Random Forest）。</p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>Boosting是一族可将弱学习器提升为强学习器的算法。这一族算法的工作机制都是类似的：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。</p>
<p>Boosting族算法最著名的代表是 AdaBoost</p>
<p>Boosting算法要求基学习器对特定的数据分布进行学习，这一点是通过“重赋权法”（re-weighting）实现的，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重，对无法接受代全样本的基学习算法，则可通过“重采样法”（re-sampling）来处理，即在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得到的样本集对基学习器进行训练。一般而言，这两种做法没有显著的优劣差别。不过由于Boosting算法在训练的每一轮都会检查当前生成的基学习器的性能是否比随机猜测好，若不符合则抛弃当前基学习器，并停止学习过程，这会导致最后的集成中只包含很少的基学习器而性能不佳。而若采用“重采样阀”，则可以获得“重启动”机会以避免训练过程的过早停止，即在抛弃不满足条件的当前基学习器之后，再根据当前分布重新对训练样本进行重采样，再基于新的采样结果重新训练出基学习器，从而使得学习过程可以持续到预设的T轮完成。</p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m此随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。由书之前的推导（式(2.1)）可知，初始训练集中越有63.2%的样本出现在采样集中。</p>
<p>于是，我们可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再集成，这就是Bagging的基本流程。在对预测输出进行结合时，Bagging通常对分类任务采用简单投票法，对回归任务使用简单平均法。若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。</p>
<p>Boosting与Bagging 相比来说最大的区别就是 Boosting是串行的，而Bagging中所有的分类器是可以同时生成的，之间没有什么关系，而Boosting中则必须先生成第一个分类器，然后根据第一个分类器的结果生成第二个分类器，依次往后进行。</p>
<h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><p>集成学习的第二类模型，为了提高集成的泛化能力，每个基学习器之间不存在很强的依赖性，所以最终预测结果时，需要一定的策略对T个结果进行结合。下面介绍结合策略。</p>
<p>平均法</p>
<p>对数值型输出，最常见的结合策略是使用平均法。简单平均法加权平均法但是对于规模比较大的集成来说，权重参数比较多，较容易导致过拟合。加权平均法未必一定优于简单平均法。一般而言，在个体学习器性能相差较大时，宜使用加权平均法，而在个体学习器性能相近时，宜使用简单平均法。</p>
<p>投票法</p>
<p>绝对多数投票法:若某标记得票过半数，则预测为该标记；否则拒绝预测。</p>
<p>相对多数投票法:预测为得票最多的标记。若同时有多个标记获得最高票，则从中随机选取一个。</p>
<p>加权投票法</p>
<p>学习法:当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。</p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>随机森林由多棵决策树组成，采用多棵决策树联合进行预测可以有效提高模型的精度。这些决策树用对训练样本集随机抽样构造出的样本集训练得到。由于训练样本集由随机抽样构造，因此称为随机森林。随机森林不仅对训练样本进行抽样，还对特征向量的分量随机抽样，在训练决策树时，每次寻找最佳分裂时只使用一部分抽样的特征分量作为候选特征进行分裂。</p>
<p>对于分类问题，一个测试样本会送到每一棵决策树中进行预测，然后进行投票，得票最多的类为最终分类结果。对于回归问题随机森林的预测输出是所有决策树输出的均值。例如随机森林有10棵决策树，有8课树的预测结果是第1类，1棵决策树的预测结果为第2类，2棵决策树的预测结果为第3类，则我们将样本判定成第1类。</p>
<p>对于n个独立同分布的随机变量xi，假设它们的方差为σ2σ2，则它们均值的方差为：</p>
<p>D(1n∑nixi)=σ2/nD(1n∑inxi)=σ2/n</p>
<p>即将多个随机变量相加取均值，方差会减小。如果将每棵决策树的输出值看作随机变量，多棵树的输出值的均值的方差会比单棵树小，因此可以降低模型的方差。由于使用了决策树进行投票，而决策树是分段常数函数，因此随机森林也是分段常数函数，是一个非线性模型，而且是判别模型</p>
<p>包外误差</p>
<p>训练每一棵决策树时有一部分样本未参与训练，可以在训练时利用这些没有被选中的样本做测试，统计它们的预测误差，称为包外误差。这种做法与交叉验证类似。二者都是把样本集切分成多份，轮流用其中的一部分样本进行训练，用剩下的样本进行测试。不同的是交叉验证把样本均匀的切分成份，在训练集中同一个样本不会出现多次；后者在每次Bootstrap抽样时同一个样本可能会被选中多次。</p>
<p>计算变量的重要性</p>
<p>随机森林有一个特点，可以在训练过程中输出变量的重要性，即哪个特征分量对分类更有用。实现的方法是置换法。它的原理是，如果某个特征分量对分类很重要，那么改变样本的该特征分量的值，样本的预测结果就容易出现错误。也就是说这个特征值对分类结果很敏感。反之，如果一个特征对分类不重要，随便改变它对分类结果没多大影响。</p>
<p>对于分类问题，训练某决策树时在包外样本集中随机挑选两个样本，如果要计算某一变量的重要性，则置换这两个样本的这个特征值。统计置换前和置换后的分类准确率。变量重要性的计算公式为：</p>
<p>v=(置换之前正确分类的样本数−置换正确分类的样本数)/OOB样本总数</p>
<p>这反应的是置换前后的分类准确率变化值。</p>
<p>上面定义的是单棵决策树的变量重要性，计算出每棵树的变量重要性之后，对该值取平均就得到随机森林的变量重要性。计算出每个变量的重要性之后，将该值归一化得到最终的重要性值。</p>
<p>总结</p>
<p>随机森林是一种集成学习算法，它将多棵决策树进行整合来完成预测。对于分类问题预测结果是所有决策树预测结果的投票；对于回归问题，是所有决策树预测结果的均值。训练时，通过Bootstrap抽样来形成每棵决策树的训练集，训练每棵决策树的每个节点时，所用的特征也是从整个特征向量中抽取的一部分特征。通过将多棵决策树集成，以及每次用采样的样本和特征分量训练每棵决策树，可以有效的降低模型的方差。</p>
<p>随机森林是一种判别模型，既支持分类问题，也支持回归问题，并且支持多分类问题。它是一种非线性模型，其预测函数为分段常数函数。</p>
<h3 id="随机森林的推广"><a href="#随机森林的推广" class="headerlink" title="随机森林的推广"></a>随机森林的推广</h3><p>随机森林的推广(Extra Trees)</p>
<p>extra trees是RF的一个变种, 原理几乎和RF一模一样，仅有区别有：</p>
<p>1） 对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，即每个决策树采用原始训练集。</p>
<p>2） 在选定了划分特征后，RF的决策树会基于信息增益，基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较的激进，他会随机的选择一个特征值来划分决策树。</p>
<p>从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，模型的方差相对于RF进一步减少，但是bias相对于RF进一步增大。在某些时候，extra trees的泛化能力比RF更好</p>
<h3 id="随机森林优缺点"><a href="#随机森林优缺点" class="headerlink" title="随机森林优缺点"></a>随机森林优缺点</h3><p>优点</p>
<ul>
<li>所有的数据都能够有效利用，而且不用人为的分出一部分数据来做cross-validation；</li>
<li>随机森林可以实现很高的精确度，但是只有很少的参数，而且对于分类和回归都适用；</li>
<li>不用担心过拟合的问题；</li>
<li>不需要事先做特征选择，每次只用随机的选取几个特征来训练树。</li>
<li>随机森林算法能解决分类与回归两种类型的问题，表现良好，由于是集成学习，方差和偏差都比较低，泛化性能优越；</li>
<li>随机森林对于高维数据集的处理能力很好，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。此外，该模型能够输出特征的重要性程度，这是一个非常实用的功能。</li>
<li>可以应对缺失数据</li>
<li>当存在分类不平衡的情况时，随机森林能够提供平衡数据集误差的有效方法</li>
<li>高度并行化，易于分布式实现f) 由于是树模型 ，不需要归一化即可之间使用</li>
</ul>
<p>缺点</p>
<ul>
<li>相比于其他算法，其输出预测可能较慢。</li>
<li>随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续型的输出。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。</li>
<li>对于许多统计建模者来说，随机森林给人的感觉像是一个黑盒子——你几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试。</li>
<li>忽略属性之间的相关性</li>
</ul>
<h3 id="Bootstrap抽样"><a href="#Bootstrap抽样" class="headerlink" title="Bootstrap抽样"></a>Bootstrap抽样</h3><p>一种数据抽样方法，它是构成Bagging算法和随机森林的基础。所谓抽样是指从一个样本数据集中随机抽取一些样本，形成新的数据集。这里有两种选择：有放回抽样和无放回抽样。对于前者，一个样本被抽中之后会放回去，在下次抽样时还有机会被抽中。对于后者，一个样本被抽中之后就从抽样集中去除，下次不会再参与抽样，因此一个样本最多只会被抽中一次。在这里Bootstrap使用的是有放回抽样。</p>
<p>假设样本集中有n个样本，每次抽中其中任何一个样本的概率都为1/n，即等概率，一个样本在每次抽样中没被抽中的概率为1-1/n。由于是有放回的抽样，每两次抽样之间是独立的，因此对于连续n次抽样，一个样本没被抽中的概率为：</p>
<p>(1−1/n)n(1−1/n)n</p>
<p>可以证明，当n趋向于无穷大时这个值的极限是1/e，约等于0.368，其中e是自然对数的底数。即如下结论成立：</p>
<p>limn→+∝（1−1/n）n=1/e     limn→+∝（1−1/n）n=1/e</p>
<p>如果样本量很大，在整个抽样过程中每个样本有0.368的概率不被抽中。由于样本集中各个样本是相互独立的，在整个抽样中所有样本大约有36.8%没有被抽中。这部分样本称为包外（Out Of Bag，简称OOB）数据</p>
<p>在Bootstrap抽样的基础上可以构造出Bagging（Bootstrap Aggregating）算法。这种方法对训练样本集进行多次Bootstrap抽样，用每次抽样形成的数据集训练一个弱学习器模型，得到多个独立的弱学习器（对于分类问题，称为弱分类器），最后用它们的组合进行预测。训练流程为：</p>
<p>循环，对i = 1, …, T</p>
<p>对训练样本集进行Bootstrap抽样，得到抽样后的训练样本集</p>
<p>用抽样得到的样本集训练一个模型hi(x)</p>
<p>结束循环</p>
<p>输出模型组合h1(x),…,hT(x)</p>
<p>其中T为弱学习器的数量。Bagging算法是一个抽象的框架，并没有指明每个弱学习器是什么类型的。如果弱学习器是决策树，这种方法就是随机森林。</p>
<p>sklearn 参数2    ​    </p>
<p>一：sklearn中决策树的参数：</p>
<p> <strong>1，criterion</strong>: ”gini” or “entropy”(default=”gini”)是计算属性的gini(基尼不纯度)还是entropy(信息增益)，来选择最合适的节点。</p>
<p> <strong>2，splitter</strong>: ”best” or “random”(default=”best”)随机选择属性还是选择不纯度最大的属性，建议用默认。</p>
<p> <strong>3，max_features</strong>: 选择最适属性时划分的特征不能超过此值。</p>
<p> 当为整数时，即最大特征数；当为小数时，训练集特征数*小数；<br> if “auto”, then max_features=sqrt(n_features).<br> If “sqrt”, thenmax_features=sqrt(n_features).<br> If “log2”, thenmax_features=log2(n_features).<br> If None, then max_features=n_features.</p>
<p> <strong>4，max_depth</strong>: (default=None)设置树的最大深度，默认为None，这样建树时，会使每一个叶节点只有一个类别，或是达到min_samples_split。</p>
<p> <strong>5，min_samples_split</strong>:根据属性划分节点时，每个划分最少的样本数。</p>
<p> <strong>6，min_samples_leaf</strong>:叶子节点最少的样本数。</p>
<p> <strong>7，max_leaf_nodes</strong>: (default=None)叶子树的最大样本数。</p>
<p> <strong>8，min_weight_fraction_leaf</strong>: (default=0) 叶子节点所需要的最小权值</p>
<p> <strong>9，verbose:</strong>(default=0) 是否显示任务进程</p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><p>二：随机森林特有的参数：</p>
<p> <strong>1，n_estimators</strong>=10：决策树的个数，越多越好，但是性能就会越差，至少100左右（具体数字忘记从哪里来的了）可以达到可接受的性能和误差率。  </p>
<p> <strong>2，bootstrap</strong>=True：是否有放回的采样。  </p>
<p> <strong>3，oob_score</strong>=False：oob（out of  band，带外）数据，即：在某次决策树训练中没有被bootstrap选中的数据。多单个模型的参数训练，我们知道可以用cross  validation（cv）来进行，但是特别消耗时间，而且对于随机森林这种情况也没有大的必要，所以就用这个数据对决策树模型进行验证，算是一个简单的交叉验证。性能消耗小，但是效果不错。   </p>
<p> <strong>4，n_jobs</strong>=1：并行job个数。这个在ensemble<a href="http://lib.csdn.net/base/datastructure" target="_blank" rel="noopener">算法</a>中非常重要，尤其是bagging（而非boosting，因为boosting的每次迭代之间有影响，所以很难进行并行化），因为可以并行从而提高性能。1=不并行；n：n个并行；-1：CPU有多少core，就启动多少job。</p>
<p> <strong>5，warm_start</strong>=False：热启动，决定是否使用上次调用该类的结果然后增加新的。  </p>
<p> <strong>6，class_weight</strong>=None：各个label的权重。  </p>
<p>   三：进行预测可以有几种形式：</p>
<p> <strong>1，predict_proba</strong>(x)：给出带有概率值的结果。每个点在所有label的概率和为1.  </p>
<p> <strong>2，predict</strong>(x)：直接给出预测结果。内部还是调用的predict_proba()，根据概率的结果看哪个类型的预测值最高就是哪个类型。  </p>
<p><strong>3，predict_log_proba</strong>(x)：和predict_proba基本上一样，只是把结果给做了log()处理</p>
<h3 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h3><p>典型的应用包括各种图像和数据的分类，人脸检测与关键点定位问题</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/01/hello-world/" rel="next" title="Hello World">
                <i class="fa fa-chevron-left"></i> Hello World
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#集成学习"><span class="nav-number">1.</span> <span class="nav-text">集成学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#个体学习器"><span class="nav-number">2.</span> <span class="nav-text">个体学习器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Boosting"><span class="nav-number">3.</span> <span class="nav-text">Boosting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bagging"><span class="nav-number">4.</span> <span class="nav-text">Bagging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结合策略"><span class="nav-number">5.</span> <span class="nav-text">结合策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机森林"><span class="nav-number">6.</span> <span class="nav-text">随机森林</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机森林的推广"><span class="nav-number">7.</span> <span class="nav-text">随机森林的推广</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机森林优缺点"><span class="nav-number">8.</span> <span class="nav-text">随机森林优缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bootstrap抽样"><span class="nav-number">9.</span> <span class="nav-text">Bootstrap抽样</span></a></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-number"></span> <span class="nav-text"> </span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实际应用"><span class="nav-number">1.</span> <span class="nav-text">实际应用</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
